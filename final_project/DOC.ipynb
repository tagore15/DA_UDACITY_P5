{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOAL  \n",
    "Enron was an American energy company who went bankrupt in early 2000 due to financial irregularities. During investigation, number of persons were found guilty and convicted. Goal of this project is to identify employees(Person of Interest-POI) who may have committed fraud using given financial and email exchange data by using machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code here is taken from project code file poi_id.py and explained in following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "import sys\n",
    "import pickle\n",
    "import pprint as pp\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY OF DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is a mapping of finance and email exchange data to person's name. We have 'poi' flag as an independent feature, which can be learnt as dependent on other features. We would learn a binomial classifier with 'poi' as predicted variable by other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF PERSONS:  146\n"
     ]
    }
   ],
   "source": [
    "print \"TOTAL NUMBER OF PERSONS: \", len(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF FEATURES:  21\n"
     ]
    }
   ],
   "source": [
    "print \"TOTAL NUMBER OF FEATURES: \", len(my_dataset[my_dataset.keys()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIST OF FEATURES:\n",
      "['salary',\n",
      " 'to_messages',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'exercised_stock_options',\n",
      " 'bonus',\n",
      " 'restricted_stock',\n",
      " 'shared_receipt_with_poi',\n",
      " 'restricted_stock_deferred',\n",
      " 'total_stock_value',\n",
      " 'expenses',\n",
      " 'loan_advances',\n",
      " 'from_messages',\n",
      " 'other',\n",
      " 'from_this_person_to_poi',\n",
      " 'poi',\n",
      " 'director_fees',\n",
      " 'deferred_income',\n",
      " 'long_term_incentive',\n",
      " 'email_address',\n",
      " 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "print \"LIST OF FEATURES:\"\n",
    "pp.pprint(my_dataset[my_dataset.keys()[1]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF PERSON OF INTEREST:  18\n",
      "COUNTS OF MISSING ENTRIES IN DIFFERENT FEATURES: \n",
      "{'bonus': 64,\n",
      " 'deferral_payments': 107,\n",
      " 'deferred_income': 97,\n",
      " 'director_fees': 129,\n",
      " 'email_address': 35,\n",
      " 'exercised_stock_options': 44,\n",
      " 'expenses': 51,\n",
      " 'from_messages': 60,\n",
      " 'from_poi_to_this_person': 60,\n",
      " 'from_this_person_to_poi': 60,\n",
      " 'loan_advances': 142,\n",
      " 'long_term_incentive': 80,\n",
      " 'other': 53,\n",
      " 'restricted_stock': 36,\n",
      " 'restricted_stock_deferred': 128,\n",
      " 'salary': 51,\n",
      " 'shared_receipt_with_poi': 60,\n",
      " 'to_messages': 60,\n",
      " 'total_payments': 21,\n",
      " 'total_stock_value': 20}\n"
     ]
    }
   ],
   "source": [
    "all_poi = 0\n",
    "num_missing_values = {}\n",
    "for name in my_dataset:\n",
    "    if my_dataset[name]['poi'] == True:\n",
    "        all_poi += 1;\n",
    "    for feature in my_dataset[name]:\n",
    "        if my_dataset[name][feature] == \"NaN\":\n",
    "            if not feature in num_missing_values:\n",
    "                num_missing_values[feature] = 1\n",
    "            else:\n",
    "                num_missing_values[feature] += 1\n",
    "\n",
    "print \"TOTAL NUMBER OF PERSON OF INTEREST: \", all_poi\n",
    "num_missing_values = {}\n",
    "for name in my_dataset:\n",
    "    for feature in my_dataset[name]:\n",
    "        if my_dataset[name][feature] == \"NaN\":\n",
    "            if not feature in num_missing_values:\n",
    "                num_missing_values[feature] = 1\n",
    "            else:\n",
    "                num_missing_values[feature] += 1\n",
    "print \"COUNTS OF MISSING ENTRIES IN DIFFERENT FEATURES: \"\n",
    "pp.pprint(num_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTLIER DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On manual inspection of data, we found that there are following persons for whom data is not valid so we would delete them from dataset.\n",
    "- LOCKHART EUGENE E  \n",
    "All feature values are undefined for this person  \n",
    "\n",
    "\n",
    "- TOTAL  \n",
    "- THE TRAVEL AGENCY IN THE PARK  \n",
    "Above 2 are are not valid persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonus': 'NaN',\n",
      " 'deferral_payments': 'NaN',\n",
      " 'deferred_income': 'NaN',\n",
      " 'director_fees': 'NaN',\n",
      " 'email_address': 'NaN',\n",
      " 'exercised_stock_options': 'NaN',\n",
      " 'expenses': 'NaN',\n",
      " 'from_messages': 'NaN',\n",
      " 'from_poi_to_this_person': 'NaN',\n",
      " 'from_this_person_to_poi': 'NaN',\n",
      " 'loan_advances': 'NaN',\n",
      " 'long_term_incentive': 'NaN',\n",
      " 'other': 'NaN',\n",
      " 'poi': False,\n",
      " 'restricted_stock': 'NaN',\n",
      " 'restricted_stock_deferred': 'NaN',\n",
      " 'salary': 'NaN',\n",
      " 'shared_receipt_with_poi': 'NaN',\n",
      " 'to_messages': 'NaN',\n",
      " 'total_payments': 'NaN',\n",
      " 'total_stock_value': 'NaN'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(my_dataset['LOCKHART EUGENE E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the outliers\n",
    "del my_dataset['LOCKHART EUGENE E']\n",
    "del my_dataset['TOTAL']\n",
    "del my_dataset['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW FEATURE CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspected list of features to find financial features of interest as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi',\n",
      " 'salary',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'long_term_incentive',\n",
      " 'loan_advances',\n",
      " 'bonus',\n",
      " 'restricted_stock',\n",
      " 'restricted_stock_deferred',\n",
      " 'total_stock_value',\n",
      " 'exercised_stock_options',\n",
      " 'deferred_income',\n",
      " 'expenses',\n",
      " 'director_fees']\n"
     ]
    }
   ],
   "source": [
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', 'long_term_incentive',\n",
    "                 'loan_advances', 'bonus', 'restricted_stock', 'restricted_stock_deferred',\n",
    "                 'total_stock_value', 'exercised_stock_options',\n",
    "                 'deferred_income', 'expenses', 'director_fees']\n",
    "pp.pprint(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For email specific features, we see that count of number of emails exchanges with poi could be low even for a poi person if total emails exchanges for that person is low.  \n",
    "To mitigate this problem, We create 2 new features\n",
    "- fraction_from_poi  =  from_poi_to_this_person/to_messages\n",
    "- fraction_to_poi = from_this_person_to_poi/from_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating new features\n",
    "def computeFraction(poi_messages, all_messages):\n",
    "    \"\"\" computes fraction of poi with give messages type and total of that message type \"\"\"\n",
    "    fraction = 0.\n",
    "    if all_messages != \"NaN\":\n",
    "        fraction = float(poi_messages)/float(all_messages)\n",
    "    else:\n",
    "        fraction = 0\n",
    "    return fraction\n",
    "\n",
    "for name in data_dict:\n",
    "    from_poi_to_this_person = my_dataset[name][\"from_poi_to_this_person\"]\n",
    "    to_messages = my_dataset[name][\"to_messages\"]\n",
    "    my_dataset[name][\"fraction_from_poi\"] = computeFraction(from_poi_to_this_person, to_messages)\n",
    "\n",
    "    from_this_person_to_poi = my_dataset[name][\"from_this_person_to_poi\"]\n",
    "    from_messages = my_dataset[name][\"from_messages\"]\n",
    "    my_dataset[name][\"fraction_to_poi\"] = computeFraction(from_this_person_to_poi, from_messages)\n",
    "features_list.append(\"fraction_from_poi\")\n",
    "features_list.append(\"fraction_to_poi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING, TUNING AND VALIDATION WITH FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of an algorithm depends upon its parameters e.g. performance of KNN algorithm would depend upon number of nearest neighbours (K). An algorithm can have number of parameters and moreover there can be multiple parameters for each step of machine learning pipeline. There can be multiple combinations corresponding to possible values of all these parameters. We need parameter tunning to find such a combination of these parameters such that performance of our algorithm becomes maximum. In below attempts with different algorithms, we use GridSearch algorithm for tuning of parameters i.e. finding parameters which maximize specified performance score metric of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('kbest', SelectKBest(k=5, score_func=<function f_classif at 0x000000001951A438>)), ('clf_dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=3,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='best'))])\n",
      "\tAccuracy: 0.82127\tPrecision: 0.28075\tRecall: 0.21800\tF1: 0.24543\tF2: 0.22820\n",
      "\tTotal predictions: 15000\tTrue positives:  436\tFalse positives: 1117\tFalse negatives: 1564\tTrue negatives: 11883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmd\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Learning code by grid search\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from tester import test_classifier\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "kbest = SelectKBest()\n",
    "\n",
    "# first we try with decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier(random_state=42)\n",
    "pipe = Pipeline([('kbest', kbest), ('clf_dt', clf_dt)])\n",
    "parameters = {'kbest__k':range(5,15), 'clf_dt__max_depth':range(2,6), 'clf_dt__min_samples_leaf':range(1,5)}\n",
    "folder = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "grid = GridSearchCV(pipe, param_grid=parameters, cv = folder, scoring = 'f1')\n",
    "grid.fit(features, labels)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Algorithm we tried above is Decision Tree classifier. As decision tree doesn't work on euclidean distances rather than depends on thresholds, so it doesn't require scaling. We also used SelectKBest module to find best features for this algorithm.  \n",
    "In order to finetune our algorithm, we used following parameters in GridSearch, which runs the algorithm on all combinations of parameters and selects best scoring algorithm.\n",
    "- k  \n",
    "specifies number of features   \n",
    "\n",
    "- max_depth  \n",
    "sepcifies maximum depth of tree  \n",
    "\n",
    "- min_samples_leaf  \n",
    "Guarantees minimum number of samples in leaf of tree  \n",
    "\n",
    "For validation of our algorithm, we used cross validation by which we split our data into training and testing set. By splitting our data, we won't overfit and could test on independent data set. Since our dataset is of small size(~140), we use stratified splits for number of iterations.  \n",
    "\n",
    "Following metrics are used for validation\n",
    "* ACCURACY  \n",
    "This is just the ratio of correctly identified categories to total number of persons. In decision tree learner above, we have got high accuracy of 0.82, but it seems to be due to skewness in data as only few persons are of interest in total data. We see True Negative counts as staggering 11883 out of 15000 total predictions. To mitigate this bias, we use precision and recall defined below.  \n",
    "\n",
    "\n",
    "* PRECISION  \n",
    "Precision is ratio of correctly identified True positive to Total Positive predictions i.e.  \n",
    "precision = True Positive(TP)/[True Positive(TP) + False Positive(FP)]  \n",
    "\n",
    "In this experiment, precision refers to fraction of person of interest correctly identified out of total identified persons of interest. If we have low precision, we are incorrectly incriminating lots of false person of interest.  \n",
    "\n",
    "We are getting moderate score of precision of 0.28 due to large numbers of False Positives (FP) with count of 1117. \n",
    "\n",
    "* RECALL  \n",
    "Recall is measure of selecting maximum number of positive samples out of total positives. i.e.  \n",
    "recall = True Positive(TP)/[True Positive(TP) + False Negative(FN)]  \n",
    "\n",
    "In this experiment, recall refers to fraction of person of interest found out of total person of interests. If we have low recall, we are missing out lots of true person of interest.\n",
    "\n",
    "We are getting very low quantity of recall (0.218) due to high numbers of false negatives (1564).\n",
    "\n",
    "We would try other algoritm K-Nearest Neighbour(KNN) to improve our score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('kbest', SelectKBest(k=5, score_func=<function f_classif at 0x000000001951A438>)), ('clf_knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.82940\tPrecision: 0.26050\tRecall: 0.15200\tF1: 0.19198\tF2: 0.16581\n",
      "\tTotal predictions: 15000\tTrue positives:  304\tFalse positives:  863\tFalse negatives: 1696\tTrue negatives: 12137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-NN Learner\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "scaler = MinMaxScaler()\n",
    "kbest = SelectKBest()\n",
    "clf_knn = KNeighborsClassifier()\n",
    "pipe = Pipeline([('scaler', scaler), ('kbest', kbest), ('clf_knn', clf_knn)])\n",
    "parameters = {'kbest__k':range(5,15), 'clf_knn__n_neighbors':range(3,10)}\n",
    "folder = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "grid = GridSearchCV(pipe, param_grid=parameters, cv = folder, scoring = 'f1')\n",
    "grid.fit(features, labels)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As nearest neighbour algorithm is distance dependent, so we have added a scaler function in pipeline above, which would standardize the feature range. For tuning the algorithm, we use number of neighbours as a parameter.  \n",
    "Following the description of evaluation metrics in Decision Tree, we see that precision and recall of KNN classifier are 0.26 and 0.15 respectively. These are very low performance, so we would try another algorithm Gaussian Naive Bayes in next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('kbest', SelectKBest(k=6, score_func=<function f_classif at 0x000000001951A438>)), ('clf_nb', GaussianNB())])\n",
      "\tAccuracy: 0.85073\tPrecision: 0.42788\tRecall: 0.35450\tF1: 0.38775\tF2: 0.36709\n",
      "\tTotal predictions: 15000\tTrue positives:  709\tFalse positives:  948\tFalse negatives: 1291\tTrue negatives: 12052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Learner\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kbest = SelectKBest()\n",
    "clf_nb = GaussianNB()\n",
    "pipe = Pipeline([('kbest', kbest), ('clf_nb', clf_nb)])\n",
    "parameters = {'kbest__k':range(5,15)}\n",
    "folder = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "grid = GridSearchCV(pipe, param_grid=parameters, cv = folder, scoring = 'f1')\n",
    "grid.fit(features, labels)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above Naive bayes classifier, we see comparitively high precision(0.42) and recall (0.35), both greater that 0.3. So we choose it as our final model.  \n",
    "As Naive Bayes doesn't depends upon distances, so no feature scaling is required.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores with different values of K i.e numbers of features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.39771, std: 0.31880, params: {'kbest__k': 5},\n",
       " mean: 0.40871, std: 0.32005, params: {'kbest__k': 6},\n",
       " mean: 0.37871, std: 0.31796, params: {'kbest__k': 7},\n",
       " mean: 0.34971, std: 0.32040, params: {'kbest__k': 8},\n",
       " mean: 0.34100, std: 0.32100, params: {'kbest__k': 9},\n",
       " mean: 0.34121, std: 0.30976, params: {'kbest__k': 10},\n",
       " mean: 0.34006, std: 0.29647, params: {'kbest__k': 11},\n",
       " mean: 0.31428, std: 0.29492, params: {'kbest__k': 12},\n",
       " mean: 0.30488, std: 0.29482, params: {'kbest__k': 13},\n",
       " mean: 0.28962, std: 0.27850, params: {'kbest__k': 14}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"scores with different values of K i.e numbers of features\"\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in above summay that score is highest corresponding to 6 feature selected by k-best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As selector is being passed in grid estimator, we explicitly find approximate features for k=6. In below results, we see that newly added feature 'fraction_to_poi' is one of feature selected for predicting final outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features selected by kbest\n",
      "['salary', 'bonus', 'total_stock_value', 'exercised_stock_options', 'deferred_income', 'fraction_to_poi']\n"
     ]
    }
   ],
   "source": [
    "kbest_exp = SelectKBest(k=6)\n",
    "kbest_exp.fit(features, labels)\n",
    "features_selected = [features_list[i+1] for i in kbest_exp.get_support(indices=True)]\n",
    "print 'features selected by kbest'\n",
    "print features_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find effect of our new feature, we find best score without our new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('kbest', SelectKBest(k=6, score_func=<function f_classif at 0x000000001951A438>)), ('clf_nb', GaussianNB())])\n",
      "\tAccuracy: 0.84600\tPrecision: 0.40264\tRecall: 0.32050\tF1: 0.35690\tF2: 0.33413\n",
      "\tTotal predictions: 15000\tTrue positives:  641\tFalse positives:  951\tFalse negatives: 1359\tTrue negatives: 12049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove 2 new features\n",
    "features_list.remove(\"fraction_from_poi\")\n",
    "features_list.remove(\"fraction_to_poi\")\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kbest = SelectKBest()\n",
    "clf_nb = GaussianNB()\n",
    "pipe = Pipeline([('kbest', kbest), ('clf_nb', clf_nb)])\n",
    "parameters = {'kbest__k':range(5,15)}\n",
    "folder = StratifiedShuffleSplit(labels, n_iter = 100, random_state = 42)\n",
    "grid = GridSearchCV(pipe, param_grid=parameters, cv = folder, scoring = 'f1')\n",
    "grid.fit(features, labels)\n",
    "clf = grid.best_estimator_\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Precision score is decreased to 0.40 from 0.42 and Recall is decreases to 0.32 from 0.35 without our new features confirming usefulness of our new feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
